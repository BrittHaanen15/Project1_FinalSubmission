{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis and Resampling Methods\n",
    "\n",
    "### Authors:\n",
    "1. Britt\n",
    "2. Pragay Shourya Moudgil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imageio import imread\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "\n",
    "    def __init__(self, y_target, y_pred):\n",
    "        \n",
    "        self.y_target = y_target\n",
    "        self.y_pred = y_pred\n",
    "\n",
    "\n",
    "    def r2_score(self):\n",
    "        a = self.y_pred - np.mean(self.y_pred)\n",
    "        b = self.y_target - np.mean(self.y_target)\n",
    "        return (np.sum(a * b)) ** 2 / (np.sum(a ** 2) * np.sum(b ** 2))\n",
    "\n",
    "            \n",
    "    \n",
    "    def mean_squared_error(self):\n",
    "        return (np.sum(np.square(self.y_target - self.y_pred ))/len(self.y_target))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y,  = x, y \n",
    "        self.frankf = self.FrankeFunction()\n",
    "\n",
    "\n",
    "    def FrankeFunction(self):\n",
    "\n",
    "        term1 = 0.75*np.exp(-(0.25*(9*self.x-2)**2) - 0.25*((9*self.y-2)**2))\n",
    "        term2 = 0.75*np.exp(-((9*self.x+1)**2)/49.0 - 0.1*(9*self.y+1))\n",
    "        term3 = 0.5*np.exp(-(9*self.x-7)**2/4.0 - 0.25*((9*self.y-3)**2))\n",
    "        term4 = -0.2*np.exp(-(9*self.x-4)**2 - (9*self.y-7)**2)\n",
    "\n",
    "        self.frankf = term1 + term2 + term3 + term4\n",
    "        \n",
    "        return self.frankf\n",
    "    \n",
    "    def plot_franke(self, z = None, name_fig = None):\n",
    "        \n",
    "        z = self.frankf\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Plot the surface.\n",
    "        surf = ax.plot_surface(self.x, self.y, z, cmap=cm.coolwarm,\n",
    "                            linewidth=0, antialiased=False)\n",
    "\n",
    "        # Customize the z axis.\n",
    "        ax.set_zlim(-0.10, 1.40)\n",
    "        ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "        # Add a color bar which maps values to colors.\n",
    "        fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "        plt.savefig(name_fig, dpi = 600, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils:\n",
    "def determine_X(poly_deg, x, y, intercept = False):\n",
    "\n",
    "    X = np.ones((len(x),int((poly_deg+1)*(poly_deg+2)/2)))\n",
    "    for i in range(1,poly_deg+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "        for k in range(i+1):\n",
    "            X[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "    if intercept is False:\n",
    "        X = X[:,1:]\n",
    "\n",
    "    \n",
    "    return X\n",
    "\n",
    "def scaling_mean(X_train, X_test):\n",
    "    mean = np.mean(X_train, axis = 0)\n",
    "    return X_train - mean, X_test - mean\n",
    "\n",
    "def prep_train_test(X, z, train_size = 0.66):\n",
    "\n",
    "    return train_test_split(X, z, train_size=train_size)\n",
    "\n",
    "def bootstrap(x,\n",
    "              y,\n",
    "              z,\n",
    "              poly_deg,\n",
    "              model\n",
    "              ):\n",
    "    #to do\n",
    "\n",
    "    pass\n",
    "\n",
    "def evaluate_stats(y_pred, y_input):\n",
    "    return [np.mean( np.mean((y_input[:, np.newaxis] - y_pred)**2, axis=1, keepdims=True)),\n",
    "            np.mean( (y_input - np.mean(y_pred, axis=1, keepdims=True))**2),\n",
    "            np.mean( np.var(y_pred, axis=1, keepdims=True))\n",
    "            ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotResults():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_polynomial(self, \n",
    "                    x_axis_val,\n",
    "                    trainvalues,\n",
    "                    testvalues = None,\n",
    "                    title = None,\n",
    "                    y_label = None,\n",
    "                    x_label = None,\n",
    "                    train_label = None,\n",
    "                    test_label = None,\n",
    "                    ):\n",
    "        # Plot data for MSE\n",
    "        plt.plot(x_axis_val, trainvalues, label=train_label)\n",
    "\n",
    "        if test_label is not None:\n",
    "            plt.plot(x_axis_val, testvalues, label=test_label)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.xticks(np.arange(x_axis_val[0], x_axis_val[-1]+1, step=1))\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS():\n",
    "\n",
    "    def __init__(self, x, y, z, poly_deg, sk_learn = False):\n",
    "        self.x, self.y, self.z, self.poly_deg, self.sk_learn = np.ravel(x), np.ravel(y), np.ravel(z), poly_deg, sk_learn\n",
    "\n",
    "    def _beta(self, X, z):\n",
    "        #calculate beta\n",
    "        return (np.linalg.inv(X.T @ X) @ X.T ) @ z\n",
    "    \n",
    "    def predict(self, X_input):\n",
    "        return X_input @ self.betahat_train\n",
    "    \n",
    "    def fit(self, train_size = 0.66):\n",
    "        \n",
    "        X = determine_X(poly_deg=self.poly_deg, x = self.x, y= self.y, intercept = False)\n",
    "\n",
    "        self.X_train, self.X_test, self.z_train, self.z_test = prep_train_test(X=X, z=self.z, train_size=train_size)\n",
    "\n",
    "        self.X_train, self.X_test = scaling_mean(self.X_train, self.X_test)\n",
    "        \n",
    "        if self.sk_learn is False:\n",
    "            self.betahat_train = self._beta(X=self.X_train, z=self.z_train)\n",
    "\n",
    "            return self.betahat_train\n",
    "        else:\n",
    "            self.model = linear_model.LinearRegression(fit_intercept=False).fit(self.X_train, self.z_train)\n",
    "            return self.model\n",
    "    \n",
    "    def evaluate(self):\n",
    "\n",
    "        if self.sk_learn is False:\n",
    "            ztilde_train = self.predict(self.X_train)\n",
    "            ztilde_test = self.predict(self.X_test)\n",
    "        \n",
    "        else:\n",
    "            ztilde_train = self.model.predict(self.X_train)\n",
    "            ztilde_test = self.model.predict(self.X_test)\n",
    "\n",
    "\n",
    "        return [Metrics(ztilde_train, self.z_train).mean_squared_error(),\n",
    "                Metrics(ztilde_test, self.z_test).mean_squared_error(),\n",
    "                Metrics(ztilde_train, self.z_train).r2_score(),\n",
    "                Metrics(ztilde_test, self.z_test).r2_score()]\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "\n",
    "    def __init__(self, x, y, z, poly_deg, ld):\n",
    "\n",
    "        self.x, self.y, self.z, self.poly_deg, self.ld = np.ravel(x), np.ravel(y), np.ravel(z), poly_deg, ld\n",
    "\n",
    "    def _beta(self, X, z, ld):\n",
    "        #calculate beta for ridge\n",
    "        xtxinv_r = np.linalg.inv((X.T.dot(X) + ld*np.identity(X.shape[1])))\n",
    "        return xtxinv_r.dot(X.T).dot(z)\n",
    "\n",
    "    def predict(self, X_input):\n",
    "        return X_input @ self.betahat_train\n",
    "    \n",
    "\n",
    "    def fit(self, train_size = 0.66):\n",
    "            \n",
    "        X = determine_X(poly_deg=self.poly_deg, x = self.x, y= self.y, intercept = False)\n",
    "\n",
    "        self.X_train, self.X_test, self.z_train, self.z_test = prep_train_test(X=X, z=self.z, train_size=train_size)\n",
    "\n",
    "        self.X_train, self.X_test = scaling_mean(self.X_train, self.X_test)\n",
    "        \n",
    "        self.betahat_train = self._beta(X=self.X_train, z=self.z_train, ld=self.ld)\n",
    "\n",
    "        return self.betahat_train\n",
    "    \n",
    "    def evaluate(self):\n",
    "\n",
    "        ztilde_train = self.predict(self.X_train)\n",
    "        ztilde_test = self.predict(self.X_test)\n",
    "\n",
    "        return [Metrics(ztilde_train, self.z_train).mean_squared_error(),\n",
    "                Metrics(ztilde_test, self.z_test).mean_squared_error(),\n",
    "                Metrics(ztilde_train, self.z_train).r2_score(),\n",
    "                Metrics(ztilde_test, self.z_test).r2_score()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression():\n",
    "\n",
    "    def __init__(self, x, y, z, poly_deg, ld):\n",
    "\n",
    "        self.x, self.y, self.z, self.poly_deg, self.ld = np.ravel(x), np.ravel(y), np.ravel(z), poly_deg, ld\n",
    "\n",
    "    def fit(self, train_size = 0.66):\n",
    "            \n",
    "        X = determine_X(poly_deg=self.poly_deg, x = self.x, y= self.y, intercept = False)\n",
    "\n",
    "        \n",
    "        self.X_train, self.X_test, self.z_train, self.z_test = prep_train_test(X=X, z=self.z, train_size=train_size)\n",
    "\n",
    "        self.model = linear_model.Lasso(alpha=self.ld, fit_intercept=False)\n",
    "\n",
    "        self.X_train, self.X_test = scaling_mean(self.X_train, self.X_test)\n",
    "        \n",
    "        self.model.fit(self.X_train, self.z_train)\n",
    "\n",
    "        print(self.model.coef_)\n",
    "\n",
    "        return self.model.coef_\n",
    "    \n",
    "    def evaluate(self):\n",
    "\n",
    "        ztilde_train = self.model.predict(self.X_train)\n",
    "        ztilde_test = self.model.predict(self.X_test)\n",
    "\n",
    "        return [Metrics(ztilde_train, self.z_train).mean_squared_error(),\n",
    "                Metrics(ztilde_test, self.z_test).mean_squared_error(),\n",
    "                Metrics(ztilde_train, self.z_train).r2_score(),\n",
    "                Metrics(ztilde_test, self.z_test).r2_score()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data.\n",
    "x = np.arange(0, 1, 0.05)\n",
    "y = np.arange(0, 1, 0.05)\n",
    "x,y = np.meshgrid(x, y)\n",
    "z = DataGenerator(x,y).FrankeFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = DataGenerator(x, y)\n",
    "temp.plot_franke(name_fig='Frankie Function.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomialvalues = np.arange(start=1, stop=6, step=1)\n",
    "OLS_MSE_trainvalues = np.zeros(len(polynomialvalues))\n",
    "OLS_MSE_testvalues = np.zeros(len(polynomialvalues))\n",
    "OLS_R2_trainvalues = np.zeros(len(polynomialvalues))\n",
    "OLS_R2_testvalues = np.zeros(len(polynomialvalues))\n",
    "Betavalues = np.zeros([len(polynomialvalues), int((polynomialvalues[-1] + 1) * (polynomialvalues[-1] + 2) / 2)])\n",
    "\n",
    "# Loop through polynomial degrees and evaluate the model\n",
    "for i, pvalue in enumerate(polynomialvalues):\n",
    "    ols_regressor = OLS(x, y, z, poly_deg=pvalue, sk_learn=False)\n",
    "    Betavalues[i, :int((i + 2) * (i + 3) / 2) - 1] = ols_regressor.fit()\n",
    "    OLS_MSE_trainvalues[i], OLS_MSE_testvalues[i], OLS_R2_trainvalues[i], \\\n",
    "        OLS_R2_testvalues[i] = ols_regressor.evaluate()\n",
    "\n",
    "beta_indexes = np.arange(start=0,stop=int((polynomialvalues[-1]+1)*(polynomialvalues[-1]+2)/2),step=1)\n",
    "Betavalues[Betavalues==0] = np.nan \n",
    "PlotResults().plot_polynomial(x_axis_val=polynomialvalues,\n",
    "                              trainvalues=OLS_MSE_trainvalues,\n",
    "                              testvalues=OLS_MSE_testvalues,\n",
    "                              title='Mean Square Error for OLS',\n",
    "                              y_label= 'MSE',\n",
    "                              x_label = 'Polynomial degree',\n",
    "                              train_label='Training',\n",
    "                              test_label='Testing',)\n",
    "\n",
    "PlotResults().plot_polynomial(x_axis_val=polynomialvalues,\n",
    "                              trainvalues=OLS_R2_trainvalues,\n",
    "                              testvalues=OLS_R2_testvalues,\n",
    "                              title='R2 for OLS',\n",
    "                              y_label= 'R2',\n",
    "                              x_label = 'Polynomial degree',\n",
    "                              train_label='Training',\n",
    "                              test_label='Testing')\n",
    "PlotResults().plot_polynomial(x_axis_val=beta_indexes,\n",
    "                              trainvalues=Betavalues.T,\n",
    "                              testvalues=None,\n",
    "                              title='Beta values for OLS',\n",
    "                              x_label='Beta Index',\n",
    "                              train_label=['p = 1',\n",
    "                                           'p = 2',\n",
    "                                           'p = 3',\n",
    "                                           'p = 4',\n",
    "                                           'p = 5'],\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labdavalues = [10**a for a in range(-5, 1)]\n",
    "polynomialvalues = [5]\n",
    "ridge_MSE_trainvalues = np.zeros(len(labdavalues))\n",
    "ridge_MSE_testvalues = np.zeros(len(labdavalues))\n",
    "ridge_R2_trainvalues = np.zeros(len(labdavalues))\n",
    "ridge_R2_testvalues = np.zeros(len(labdavalues))\n",
    "Betavalues = np.zeros([len(labdavalues), int((polynomialvalues[-1] + 1) * (polynomialvalues[-1] + 2) / 2)-1])\n",
    "\n",
    "# Loop through polynomial degrees and evaluate the model\n",
    "for i, ld in enumerate(labdavalues):\n",
    "    ridge_regressor = RidgeRegression(x, y, z, poly_deg=polynomialvalues[-1], ld=ld)\n",
    "    Betavalues[i] = ridge_regressor.fit()\n",
    "    ridge_MSE_trainvalues[i], ridge_MSE_testvalues[i], ridge_R2_trainvalues[i], \\\n",
    "        ridge_R2_testvalues[i] = ridge_regressor.evaluate()\n",
    "    \n",
    "beta_indexes = np.arange(start=0,stop=int((polynomialvalues[-1]+1)*(polynomialvalues[-1]+2)/2 - 1),step=1)\n",
    "Betavalues[Betavalues==0] = np.nan \n",
    "PlotResults().plot_polynomial(x_axis_val= np.log10(labdavalues),\n",
    "                              trainvalues=ridge_MSE_trainvalues,\n",
    "                              testvalues=ridge_MSE_testvalues,\n",
    "                              title='Mean Square Error for Ridge with p=5',\n",
    "                              y_label= 'MSE',\n",
    "                              x_label = 'log10(Lambda Values)',\n",
    "                              train_label='Training',\n",
    "                              test_label='Testing')\n",
    "\n",
    "PlotResults().plot_polynomial(x_axis_val= np.log10(labdavalues),\n",
    "                              trainvalues=ridge_R2_trainvalues,\n",
    "                              testvalues=ridge_R2_testvalues,\n",
    "                              title='R2 for Ridge with p=5',\n",
    "                              y_label= 'R2',\n",
    "                              x_label = 'log10(Lambda Values)',\n",
    "                              train_label='Training',\n",
    "                              test_label='Testing')\n",
    "PlotResults().plot_polynomial(x_axis_val=beta_indexes,\n",
    "                              trainvalues=Betavalues.T,\n",
    "                              testvalues=None,\n",
    "                              title='Beta values for Ridge with p=5',\n",
    "                              x_label='Beta Index',\n",
    "                              train_label=labdavalues,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labdavalues = [10**a for a in range(-5, 1)]\n",
    "polynomialvalues = [5]\n",
    "lasso_MSE_trainvalues = np.zeros(len(labdavalues))\n",
    "lasso_MSE_testvalues = np.zeros(len(labdavalues))\n",
    "lasso_R2_trainvalues = np.zeros(len(labdavalues))\n",
    "lasso_R2_testvalues = np.zeros(len(labdavalues))\n",
    "\n",
    "\n",
    "# Loop through polynomial degrees and evaluate the model\n",
    "for i, ld in enumerate(labdavalues):\n",
    "    lasso_regressor = LassoRegression(x, y, z, poly_deg=polynomialvalues[-1], ld=ld)\n",
    "    Betavalues[i] = lasso_regressor.fit()\n",
    "    lasso_MSE_trainvalues[i], lasso_MSE_testvalues[i], lasso_R2_trainvalues[i], \\\n",
    "        lasso_R2_testvalues[i] = lasso_regressor.evaluate()\n",
    "\n",
    "beta_indexes = np.arange(start=0,stop=int((polynomialvalues[-1]+1)*(polynomialvalues[-1]+2)/2 - 1),step=1)\n",
    "Betavalues[Betavalues==0] = np.nan \n",
    "\n",
    "PlotResults().plot_polynomial(x_axis_val= np.log10(labdavalues),\n",
    "                              trainvalues=lasso_MSE_trainvalues,\n",
    "                              testvalues=lasso_MSE_testvalues,\n",
    "                              title='Mean Square Error for Lasso with p=5',\n",
    "                              y_label= 'MSE',\n",
    "                              x_label = 'log10(Lambda values)',\n",
    "                              train_label='Training',\n",
    "                              test_label='Testing')\n",
    "\n",
    "PlotResults().plot_polynomial(x_axis_val= np.log10(labdavalues),\n",
    "                              trainvalues=lasso_R2_trainvalues,\n",
    "                              testvalues=lasso_R2_testvalues,\n",
    "                              title='R2 for Lasso with p=5',\n",
    "                              y_label= 'R2',\n",
    "                              x_label = 'log10(Lambda Values)',\n",
    "                              train_label='Training',\n",
    "                              test_label='Testing')\n",
    "PlotResults().plot_polynomial(x_axis_val=beta_indexes,\n",
    "                              trainvalues=Betavalues.T,\n",
    "                              testvalues=None,\n",
    "                              title='Beta values for Lasso with p=5',\n",
    "                              x_label='Beta Index',\n",
    "                              train_label=labdavalues,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "n = 40\n",
    "n_boostraps = 100\n",
    "maxdegree = 14\n",
    "polynomialvalues = np.arange(start=1, stop=maxdegree+1, step=1)\n",
    "\n",
    "error = np.zeros(maxdegree)\n",
    "bias = np.zeros(maxdegree)\n",
    "variance = np.zeros(maxdegree)\n",
    "\n",
    "\n",
    "for i, deg in tqdm(enumerate(polynomialvalues), ncols = 50, total=len(polynomialvalues)):\n",
    "    X = determine_X(poly_deg=deg, x = np.ravel(x), y = np.ravel(y), intercept=False)\n",
    "    X_train, X_test, z_train, z_test = prep_train_test(X=X, z=np.ravel(z), train_size=0.66)\n",
    "\n",
    "    X_train, X_test = scaling_mean(X_train, X_test)\n",
    "    model = linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "    z_pred = np.empty((z_test.shape[0], n_boostraps))\n",
    "    for j in range(n_boostraps):\n",
    "        X_, z_ = resample(X_train, z_train)\n",
    "        z_pred[:, j] = model.fit(X_, z_).predict(X_test)\n",
    "\n",
    "    error[i] = np.mean( np.mean((z_test[:, np.newaxis] - z_pred)**2, axis=1, keepdims=True) )\n",
    "    bias[i] = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )\n",
    "    variance[i] = np.mean( np.var(z_pred, axis=1, keepdims=True) )\n",
    "\n",
    "plt.plot(polynomialvalues, error, label='Error')\n",
    "plt.plot(polynomialvalues, bias, label='bias')\n",
    "plt.plot(polynomialvalues, variance, label='Variance')\n",
    "plt.title(\"Error, bias, and variance versus polynomial degree (Bootstrap)\")\n",
    "plt.xticks(np.arange(0, maxdegree+1, step=1))\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdegree = 14\n",
    "k = 10\n",
    "polynomialvalues = np.arange(start=1, stop=maxdegree+1, step=1)\n",
    "\n",
    "# Initialize a KFold instance\n",
    "kfold = KFold(n_splits = k)\n",
    "\n",
    "# Perform the cross-validation to estimate MSE\n",
    "Errors_KFold = np.zeros((maxdegree, k))\n",
    "Biases_KFold = np.zeros((maxdegree, k))\n",
    "Variances_KFold = np.zeros((maxdegree, k))\n",
    "\n",
    "z_copy = np.ravel(z)\n",
    "\n",
    "for i, p in enumerate(polynomialvalues):\n",
    "    model = linear_model.LinearRegression(fit_intercept=False)\n",
    "    X = determine_X(x=np.ravel(x), y=np.ravel(y), poly_deg=p)\n",
    "    for j, (train_inds, test_inds) in enumerate(kfold.split(X)):\n",
    "        xtrain = X[train_inds]\n",
    "        ztrain = z_copy[train_inds]\n",
    "\n",
    "        xtest = X[test_inds]\n",
    "        ztest = z_copy[test_inds]\n",
    "        model.fit(xtrain, ztrain[:, np.newaxis])\n",
    "\n",
    "        zpred = model.predict(xtest)\n",
    "\n",
    "        Errors_KFold[i,j] = np.mean((ztest-zpred)**2)\n",
    "        Biases_KFold[i,j] = np.mean((ztest - np.mean(zpred))**2)\n",
    "        Variances_KFold[i,j] = np.mean((zpred - np.mean(zpred))**2)\n",
    "\n",
    "\n",
    "Error = np.mean(Errors_KFold, axis = 1)\n",
    "Bias = np.mean(Biases_KFold, axis = 1)\n",
    "Variance = np.mean(Variances_KFold, axis = 1)\n",
    "\n",
    "\n",
    "## Plot\n",
    "plt.figure()\n",
    "plt.plot(np.arange(start=1, stop=p+1, step=1), Error, label = 'Error')\n",
    "plt.plot(np.arange(start=1, stop=p+1, step=1), Bias, label = 'Bias')\n",
    "plt.plot(np.arange(start=1, stop=p+1, step=1), Variance, label = 'Variance')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.xticks(np.arange(0, maxdegree+1, step=1))\n",
    "plt.title(\"Error, Bias, and Variance versus polynomial degree (k-fold)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdegree = 14\n",
    "\n",
    "polynomialvalues = np.arange(start=1, stop=maxdegree+1, step=1)\n",
    "\n",
    "# Initialize a KFold instance\n",
    "kvalues = np.arange(start=5, stop=11, step=1)\n",
    "\n",
    "# Perform the cross-validation to estimate MSE\n",
    "Errors_KFold = np.zeros((maxdegree, k))\n",
    "Biases_KFold = np.zeros((maxdegree, k))\n",
    "Variances_KFold = np.zeros((maxdegree, k))\n",
    "Errors=np.zeros(len(kvalues))\n",
    "z_copy = np.ravel(z)\n",
    "\n",
    "for m, k in enumerate(kvalues):\n",
    "    kfold = KFold(n_splits = k)\n",
    "    for i, p in enumerate(polynomialvalues):\n",
    "        model = linear_model.LinearRegression(fit_intercept=False)\n",
    "        X = determine_X(x=np.ravel(x), y=np.ravel(y), poly_deg=p)\n",
    "        for j, (train_inds, test_inds) in enumerate(kfold.split(X)):\n",
    "            xtrain = X[train_inds]\n",
    "            ztrain = z_copy[train_inds]\n",
    "\n",
    "            xtest = X[test_inds]\n",
    "            ztest = z_copy[test_inds]\n",
    "            model.fit(xtrain, ztrain[:, np.newaxis])\n",
    "\n",
    "            zpred = model.predict(xtest)\n",
    "\n",
    "            Errors_KFold[i,j] = np.mean((ztest-zpred)**2)\n",
    "\n",
    "\n",
    "    Errors[m] = np.max(np.mean(Errors_KFold, axis = 1))\n",
    "\n",
    "plt.plot(kvalues, Errors)\n",
    "plt.title(\"Maximum observed error versus k value\")\n",
    "plt.xlabel('K Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdegree = 14\n",
    "\n",
    "polynomialvalues = np.arange(start=1, stop=maxdegree+1, step=1)\n",
    "\n",
    "# Initialize a KFold instance\n",
    "kvalues = 5\n",
    "\n",
    "lambdas = [10**x for x in range(-6, 2, 1)]\n",
    "\n",
    "f = plt.subplots(2, round(len(lambdas)/2), figsize=(20,10),sharey=True)\n",
    "\n",
    "for m ,lmb in enumerate(lambdas):\n",
    "    for i, p in enumerate(polynomialvalues):\n",
    "        model = linear_model.Ridge(alpha=lmb)\n",
    "        X = determine_X(x=np.ravel(x), y=np.ravel(y), poly_deg=p)\n",
    "        for j, (train_inds, test_inds) in enumerate(kfold.split(X)):\n",
    "            xtrain = X[train_inds]\n",
    "            ztrain = z_copy[train_inds]\n",
    "\n",
    "            xtest = X[test_inds]\n",
    "            ztest = z_copy[test_inds]\n",
    "            model.fit(xtrain, ztrain[:, np.newaxis])\n",
    "\n",
    "            zpred = model.predict(xtest)\n",
    "\n",
    "            Errors_KFold[i,j] = np.mean((ztest-zpred)**2)\n",
    "            Biases_KFold[i,j] = np.mean((ztest - np.mean(zpred))**2)\n",
    "            Variances_KFold[i,j] = np.mean((zpred - np.mean(zpred))**2)\n",
    "\n",
    "    Error = np.mean(Errors_KFold, axis = 1)\n",
    "    Bias = np.mean(Biases_KFold, axis = 1)\n",
    "    Variance = np.mean(Variances_KFold, axis = 1)\n",
    "    \n",
    "\n",
    "    ## Plot\n",
    "    plt.subplot(2, round(len(lambdas)/2), m+1)\n",
    "    plt.plot(np.arange(start=1, stop=p+1, step=1), Error, label = 'Error')\n",
    "    plt.plot(np.arange(start=1, stop=p+1, step=1), Bias, label = 'Bias')\n",
    "    plt.plot(np.arange(start=1, stop=p+1, step=1), Variance, label = 'Variance')\n",
    "    plt.xlabel('Polynomial degree')\n",
    "    plt.title(\"Hyperparameter = \" + str(lmb))\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "plt.suptitle(\"Error, bias, and variance versus polynomial degree (Ridge)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdegree = 14\n",
    "\n",
    "polynomialvalues = np.arange(start=1, stop=maxdegree+1, step=1)\n",
    "\n",
    "# Initialize a KFold instance\n",
    "kvalues = 5\n",
    "\n",
    "lambdas = [10**x for x in range(-6, 2, 1)]\n",
    "\n",
    "f = plt.subplots(2, round(len(lambdas)/2), figsize=(20,10),sharey=True)\n",
    "\n",
    "for m ,lmb in enumerate(lambdas):\n",
    "    for i, p in enumerate(polynomialvalues):\n",
    "        model = linear_model.Lasso(alpha=lmb)\n",
    "        X = determine_X(x=np.ravel(x), y=np.ravel(y), poly_deg=p)\n",
    "        for j, (train_inds, test_inds) in enumerate(kfold.split(X)):\n",
    "            xtrain = X[train_inds]\n",
    "            ztrain = z_copy[train_inds]\n",
    "\n",
    "            xtest = X[test_inds]\n",
    "            ztest = z_copy[test_inds]\n",
    "            model.fit(xtrain, ztrain[:, np.newaxis])\n",
    "\n",
    "            zpred = model.predict(xtest)\n",
    "\n",
    "            Errors_KFold[i,j] = np.mean((ztest-zpred)**2)\n",
    "            Biases_KFold[i,j] = np.mean((ztest - np.mean(zpred))**2)\n",
    "            Variances_KFold[i,j] = np.mean((zpred - np.mean(zpred))**2)\n",
    "\n",
    "    Error = np.mean(Errors_KFold, axis = 1)\n",
    "    Bias = np.mean(Biases_KFold, axis = 1)\n",
    "    Variance = np.mean(Variances_KFold, axis = 1)\n",
    "    \n",
    "\n",
    "    ## Plot\n",
    "    plt.subplot(2, round(len(lambdas)/2), m+1)\n",
    "    plt.plot(np.arange(start=1, stop=p+1, step=1), Error, label = 'Error')\n",
    "    plt.plot(np.arange(start=1, stop=p+1, step=1), Bias, label = 'Bias')\n",
    "    plt.plot(np.arange(start=1, stop=p+1, step=1), Variance, label = 'Variance')\n",
    "    plt.xlabel('Polynomial degree')\n",
    "    plt.title(\"Hyperparameter = \" + str(lmb))\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "plt.suptitle(\"Error, bias, and variance versus polynomial degree (Lasso)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_dataset('SRTM_data_Norway_1.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['band_data'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the terrain\n",
    "terrain1 = imread('SRTM_data_Norway_1.tif')\n",
    "# Show the terrain\n",
    "plt.figure()\n",
    "plt.title('Terrain over Norway 1')\n",
    "plt.imshow(terrain1, cmap='gray', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Terrain over Norway.png', dpi = 600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(0, terrain1.shape[0], step = 1)\n",
    "y = np.arange(0, terrain1.shape[1], step = 1)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = terrain1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming determine_X, z, x, and y are defined\n",
    "z_copy = np.ravel(z)\n",
    "\n",
    "\n",
    "# Custom scorer to handle polynomial degree\n",
    "def custom_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return mean_squared_error(y, y_pred)\n",
    "\n",
    "# Initialize the Ridge regression model\n",
    "linear = linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Initialize the KFold cross-validator\n",
    "kfold = KFold(n_splits=10)\n",
    "z_copy = np.ravel(z)\n",
    "# Perform grid search\n",
    "best_params = []\n",
    "for degree in tqdm(polynomialvalues, ncols=50, total=len(polynomialvalues)):\n",
    "    X = determine_X(x=np.ravel(x), y=np.ravel(y), poly_deg=degree, intercept=False)\n",
    "    X_train, X_test, z_train, z_test = prep_train_test(X=X, z=z_copy, train_size=0.66)\n",
    "    X_train, X_test = scaling_mean(X_train, X_test)\n",
    "    grid_search = GridSearchCV(estimator=linear, param_grid={}, scoring=custom_scorer, cv=kfold, n_jobs=-1)\n",
    "    grid_search.fit(X_train, z_train)\n",
    "\n",
    "    best_params.append((degree, grid_search.score(X_test, z_test)))\n",
    "\n",
    "# Extract the best parameters\n",
    "best_params_linear = np.array(best_params)\n",
    "best_degrees_linear = best_params_linear[:, 0]\n",
    "best_scores_linear = best_params_linear[:, 1]\n",
    "\n",
    "# Plotting the results\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(best_degrees_linear, best_scores_linear, marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Best Mean Squared Error for each Polynomial Degree')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "z_copy = np.ravel(z)\n",
    "\n",
    "# Precompute polynomial features for all degrees\n",
    "maxdegree = 15\n",
    "polynomialvalues = np.arange(start=1, stop=maxdegree+1, step=1)\n",
    "X_poly = [determine_X(x=np.ravel(x), y=np.ravel(y), poly_deg=p, intercept=False) for p in polynomialvalues]\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [10**x for x in range(-6, 2, 1)],\n",
    "    'degree': polynomialvalues\n",
    "}\n",
    "\n",
    "# Custom scorer to handle polynomial degree\n",
    "def custom_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return mean_squared_error(y, y_pred)\n",
    "\n",
    "# Initialize the Ridge regression model\n",
    "ridge = linear_model.Ridge(fit_intercept=False)\n",
    "\n",
    "# Initialize the KFold cross-validator\n",
    "kfold = KFold(n_splits=10)\n",
    "z_copy = np.ravel(z)\n",
    "# Perform grid search\n",
    "best_params = []\n",
    "for degree in tqdm(polynomialvalues, ncols=50, total=len(polynomialvalues)):\n",
    "    X = determine_X(x=np.ravel(x), y=np.ravel(y), poly_deg=degree, intercept=False)\n",
    "    X_train, X_test, z_train, z_test = prep_train_test(X=X, z=z_copy, train_size=0.66)\n",
    "    X_train, X_test = scaling_mean(X_train, X_test)\n",
    "    grid_search = GridSearchCV(estimator=ridge, param_grid={'alpha': param_grid['alpha']}, scoring=custom_scorer, cv=kfold, n_jobs=-1)\n",
    "    grid_search.fit(X_train, z_train)\n",
    "\n",
    "    best_params.append((degree, grid_search.best_params_['alpha'], grid_search.score(X_test, z_test)))\n",
    "\n",
    "# Extract the best parameters\n",
    "best_params_ridge = np.array(best_params)\n",
    "best_degrees_ridge = best_params_ridge[:, 0]\n",
    "best_alphas_ridge = best_params_ridge[:, 1]\n",
    "best_scores_ridge = best_params_ridge[:, 2]\n",
    "\n",
    "# Plotting the results\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(best_degrees_ridge, best_scores_ridge, marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Best Mean Squared Error for each Polynomial Degree')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_copy = np.ravel(z)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [10**x for x in range(-6, 1, 1)],\n",
    "    'degree': polynomialvalues\n",
    "}\n",
    "\n",
    "# Custom scorer to handle polynomial degree\n",
    "def custom_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return mean_squared_error(y, y_pred)\n",
    "\n",
    "# Initialize the Ridge regression model\n",
    "lasso = linear_model.Lasso(fit_intercept=False, tol=0.01)\n",
    "\n",
    "# Initialize the KFold cross-validator\n",
    "kfold = KFold(n_splits=10)\n",
    "z_copy = np.ravel(z)\n",
    "# Perform grid search\n",
    "best_params = []\n",
    "scaler = StandardScaler(with_std=False)\n",
    "for degree in tqdm(polynomialvalues, ncols=50, total=len(polynomialvalues)):\n",
    "    X = determine_X(x=np.ravel(x), y=np.ravel(y), poly_deg=degree, intercept=False)\n",
    "    X_train, X_test, z_train, z_test = prep_train_test(X=X, z=z_copy, train_size=0.66)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    grid_search = GridSearchCV(estimator=lasso, param_grid={'alpha': param_grid['alpha']}, scoring=custom_scorer, cv=kfold, n_jobs=-1)\n",
    "    grid_search.fit(X_train, z_train)\n",
    "\n",
    "    best_params.append((degree, grid_search.best_params_['alpha'], grid_search.score(X_test, z_test)))\n",
    "\n",
    "# Extract the best parameters\n",
    "best_params_lasso = np.array(best_params)\n",
    "best_degrees_lasso = best_params_lasso[:, 0]\n",
    "best_alphas_lasso = best_params_lasso[:, 1]\n",
    "best_scores_lasso = best_params_lasso[:, 2]\n",
    "\n",
    "# Plotting the results\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(best_degrees_lasso, best_scores_lasso, marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Best Mean Squared Error for each Polynomial Degree')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE of OLS:', np.min(best_scores_linear))\n",
    "print('Best polynomial degree for OLS:', best_degrees_linear[np.argmin(best_scores_linear)])\n",
    "\n",
    "print('MSE of Ridge:', np.min(best_scores_ridge))\n",
    "print('Best polynomial degree for Ridge:', best_degrees_ridge[np.argmin(best_scores_ridge)])\n",
    "print('Best alpha for Ridge:', best_alphas_ridge[np.argmin(best_scores_ridge)])\n",
    "\n",
    "print('MSE of Lasso:', np.min(best_scores_lasso))\n",
    "print('Best polynomial degree for Lasso:', best_degrees_lasso[np.argmin(best_scores_lasso)])\n",
    "print('Best alpha for Lasso:', best_alphas_lasso[np.argmin(best_scores_lasso)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters for ridge and lasso:\n",
    "print('Hyperparameters for Linear:', pd.DataFrame(best_params_linear, columns=['Degree', 'MSE']))\n",
    "print('Hyperparameters for Ridge:', pd.DataFrame(best_params_ridge, columns=['Degree', 'Alpha', 'MSE']))\n",
    "print('Hyperparameters for Lasso:', pd.DataFrame(best_params_lasso, columns=['Degree', 'Alpha', 'MSE']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
